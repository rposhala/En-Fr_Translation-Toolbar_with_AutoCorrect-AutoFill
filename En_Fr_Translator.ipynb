{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "En-Fr-Translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EjIikB8FBit2Q7HuNM6En9GQQql8KVST",
      "authorship_tag": "ABX9TyP56nWcuyAQozTBgX63bPRn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rposhala/En-Fr_Translation-Toolbar_with_AutoCorrect-AutoFill/blob/development/En_Fr_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R6BEbZLgZn0"
      },
      "source": [
        "import pickle\n",
        "import string\n",
        "import gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvroA3ATiMz6"
      },
      "source": [
        "def get_dict(file_name):\n",
        "    \"\"\"\n",
        "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
        "    Check out the files this function takes in your workspace.\n",
        "    \"\"\"\n",
        "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
        "    etof = {}  # the english to french dictionary to be returned\n",
        "    for i in range(len(my_file)):\n",
        "        # indexing into the rows.\n",
        "        en = my_file.loc[i][0]\n",
        "        fr = my_file.loc[i][1]\n",
        "        etof[en] = fr\n",
        "\n",
        "    return etof\n",
        "\n",
        "\n",
        "def cosine_similarity(A, B):\n",
        "    '''\n",
        "    Input: A: a numpy array which corresponds to a word vector\n",
        "          B: A numpy array which corresponds to a word vector\n",
        "    Output: cos: numerical number representing the cosine similarity between A and B.\n",
        "    '''\n",
        "    # you have to set this variable to the true label.\n",
        "    cos = -10\n",
        "    dot = np.dot(A, B)\n",
        "    norma = np.linalg.norm(A)\n",
        "    normb = np.linalg.norm(B)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    return cos\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUEPWI1CoEs_"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "\n",
        "# The word embeddings data for English and French words\n",
        "\n",
        "Data: The full dataset for English embeddings is about 3.64 gigabytes, and the French embeddings are about 629 megabytes. \n",
        "\n",
        "* English embeddings are downloaded from Google code archive word2vec and unzipped\n",
        "[GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
        "\n",
        "* and the French embeddings are downloaded from\n",
        "[cross_lingual_text_classification](https://github.com/vjstark/crosslingual_text_classification).\n",
        " it done through the terminal, by typing (in one line)\n",
        "    `curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoToH7r08j-Y",
        "outputId": "dc1f0868-bb74-40de-9040-f4bd32910c01"
      },
      "source": [
        "# I used my google drive manage the dataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/MyDrive/Colab Notebooks/NLP_Specialization\")\n",
        "!ls\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Course2-NLPProbModels  GoogleNews-vectors-negative300.bin\n",
            "en_embeddings.p        GoogleNews-vectors-negative300.bin.gz\n",
            "en-fr.test.txt\t       NLP_with_Classification_and_Vector_Spaces\n",
            "en-fr.train.txt        project.ipynb\n",
            "fr_embeddings.p        wiki.multi.fr.vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke-T3eNN3h45",
        "outputId": "af26f7f6-70e4-4117-df7d-bd77ad46d2de"
      },
      "source": [
        "\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
        "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n",
        "print(\"English and French embeddings are loaded\")\n",
        "\n",
        "# loading the english to french dictionaries\n",
        "en_fr_train = get_dict('./en-fr.train.txt')\n",
        "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
        "en_fr_test = get_dict('./en-fr.test.txt')\n",
        "print('The length of the english to french test dictionary is', len(en_fr_train))\n",
        "\n",
        "english_set = set(en_embeddings.vocab)\n",
        "french_set = set(fr_embeddings.vocab)\n",
        "en_embeddings_subset = {}\n",
        "fr_embeddings_subset = {}\n",
        "french_words = set(en_fr_train.values())\n",
        "\n",
        "for en_word in en_fr_train.keys():\n",
        "    fr_word = en_fr_train[en_word]\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "\n",
        "for en_word in en_fr_test.keys():\n",
        "    fr_word = en_fr_test[en_word]\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "\n",
        "pickle.dump( en_embeddings_subset, open( \"./en_embeddings.p\", \"wb\" ) )\n",
        "pickle.dump( fr_embeddings_subset, open( \"./fr_embeddings.p\", \"wb\" ) )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English and French embeddings are loaded\n",
            "The length of the english to french training dictionary is 5000\n",
            "The length of the english to french test dictionary is 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re1nT4J0gn4E"
      },
      "source": [
        "en_embeddings_subset = pickle.load(open(\"./en_embeddings.p\", \"rb\"))\n",
        "fr_embeddings_subset = pickle.load(open(\"./fr_embeddings.p\", \"rb\"))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFXAmQxpiqu4"
      },
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        en_fr: English to French dictionary\n",
        "        french_vecs: French words to their corresponding word embeddings.\n",
        "        english_vecs: English words to their corresponding word embeddings.\n",
        "    Output: \n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
        "    \"\"\"\n",
        "\n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "\n",
        "    english_set = set(english_vecs.keys())\n",
        "\n",
        "    french_set = set(french_vecs.keys())\n",
        "    french_words = set(en_fr.values())\n",
        "\n",
        "    for en_word, fr_word in en_fr.items():\n",
        "        if fr_word in french_set and en_word in english_set:\n",
        "            en_vec = english_vecs[en_word]\n",
        "            fr_vec = french_vecs[fr_word]\n",
        "            X_l.append(en_vec)\n",
        "            Y_l.append(fr_vec)\n",
        "    X = np.stack(X_l)\n",
        "    Y = np.stack(Y_l)\n",
        "\n",
        "    return X, Y\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8FKRoFEiqrr"
      },
      "source": [
        "# getting the training set:\n",
        "X_train, Y_train = get_matrices(\n",
        "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29hRWuMZgn1E"
      },
      "source": [
        "def compute_loss(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
        "    '''\n",
        "    m = len(X)\n",
        "    # diff is XR - Y\n",
        "    diff = np.dot(X, R) - Y\n",
        "    diff_squared = np.square(diff)\n",
        "    sum_diff_squared = np.sum(diff_squared)\n",
        "    loss = sum_diff_squared/m\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USU-OAkwi70w"
      },
      "source": [
        "def compute_gradient(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
        "    '''\n",
        "    m = len(X)\n",
        "\n",
        "    # gradient is X^T(XR - Y) * 2/m\n",
        "    gradient = (2*np.dot(X.T, (np.dot(X, R) - Y)))/m\n",
        "\n",
        "    return gradient\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ethNc36SgthP"
      },
      "source": [
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
        "    Outputs:\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
        "    '''\n",
        "    np.random.seed(130)\n",
        "\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if i % 20 == 0:\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        gradient = compute_gradient(X, Y, R)\n",
        "        R -= learning_rate*gradient\n",
        "\n",
        "    return R\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYQJXaTmgtcn",
        "outputId": "8e3e3c33-d78a-4605-ed8d-e103aec7eaea"
      },
      "source": [
        "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at iteration 0 is: 962.0391\n",
            "loss at iteration 20 is: 133.0551\n",
            "loss at iteration 40 is: 43.0979\n",
            "loss at iteration 60 is: 17.4929\n",
            "loss at iteration 80 is: 8.2717\n",
            "loss at iteration 100 is: 4.4266\n",
            "loss at iteration 120 is: 2.6432\n",
            "loss at iteration 140 is: 1.7458\n",
            "loss at iteration 160 is: 1.2646\n",
            "loss at iteration 180 is: 0.9935\n",
            "loss at iteration 200 is: 0.8346\n",
            "loss at iteration 220 is: 0.7384\n",
            "loss at iteration 240 is: 0.6787\n",
            "loss at iteration 260 is: 0.6408\n",
            "loss at iteration 280 is: 0.6163\n",
            "loss at iteration 300 is: 0.6001\n",
            "loss at iteration 320 is: 0.5893\n",
            "loss at iteration 340 is: 0.5820\n",
            "loss at iteration 360 is: 0.5769\n",
            "loss at iteration 380 is: 0.5734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABAW8KWMgtZr"
      },
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    similarity_l = []\n",
        "\n",
        "    for row in candidates:\n",
        "        cos_similarity = cosine_similarity(v, row)\n",
        "        similarity_l.append(cos_similarity)\n",
        "    \n",
        "    sorted_ids = np.argsort(similarity_l)\n",
        "    k_idx = sorted_ids[len(sorted_ids)-k:]\n",
        "    \n",
        "    return k_idx\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgdCPgXNjYPF"
      },
      "source": [
        "def test_vocabulary(X, Y, R):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the transform matrix which translates word embeddings from\n",
        "        English to French word vector space.\n",
        "    Output:\n",
        "        accuracy: for the English to French capitals\n",
        "    '''\n",
        "    pred = np.dot(X, R)\n",
        "\n",
        "    num_correct = 0\n",
        "\n",
        "    for i in range(len(pred)):\n",
        "        pred_idx = nearest_neighbor(pred[i], Y, 1)\n",
        "\n",
        "        if pred_idx == i:\n",
        "            num_correct += 1\n",
        "    accuracy = num_correct/len(X)\n",
        "\n",
        "    return accuracy\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuP83YfdjX-y"
      },
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otlk5cXLjX7y",
        "outputId": "9796aaac-c662-4fcf-f2d3-f36f193d1b47"
      },
      "source": [
        "acc = test_vocabulary(X_val, Y_val, R_train)\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test set is 0.552\n"
          ]
        }
      ]
    }
  ]
}